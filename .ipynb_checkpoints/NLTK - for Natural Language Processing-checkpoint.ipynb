{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK for NLP \n",
    "it is a Python module to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet and text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus - a large collection of text <br>\n",
    "corpora - plural of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown is one of the corpus offered by NLTK. This corpus contains text from 500 sources, and \n",
    "# the sources have been categorized by genre, such as news, editorial, and so on.\n",
    "# one can see examples ,sentences etc of any category using brown corpus.\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "15\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "data=brown.categories()\n",
    "print(type(data),len(data),data,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thirty-three\n",
      "Scotty did not go back to school .\n",
      "His parents talked seriously and lengthily to their own doctor and to a specialist at the University Hospital -- Mr. McKinley was entitled to a discount for members of his family -- and it was decided it would be best for him to take the remainder of the term off , spend a lot of time in bed and , for the rest , do pretty much as he chose -- provided , of course , he chose to do nothing too exciting or too debilitating .\n",
      "His teacher and his school principal were conferred with and everyone agreed that , if he kept up with a certain amount of work at home , there was little danger of his losing a term .\n",
      "Scotty accepted the decision with indifference and did not enter the arguments .\n"
     ]
    }
   ],
   "source": [
    "data=brown.sents(categories='fiction')\n",
    "for i in range(5):\n",
    "    print(' '.join(data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words pipeline\n",
    "Machine learning classifiers work on numbers and not on text so to carry out tasks like movie rating prediction based on reviews one has only reviews in human readable language. To perform processing on text data , text has to be converted into numbers using bag of words model so that ML classifier can work on it.\n",
    "<ul>\n",
    "    <li>\n",
    "Get the Data/Corpus    </li>    <li>\n",
    "Tokenisation, Stopward Removal    </li>    <li>\n",
    "Stemming/Lemmatisation   </li>    <li>\n",
    "Building a Vocab   </li>    <li>\n",
    "Vectorization - converting sentence to a vector of numbers   </li>    <li>\n",
    "Classification    </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers.\n",
    "I went to the market to buy some fruits.\"\"\"\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at Harshit@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to buy some fruits.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(document)\n",
    "print(sents)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'Harshit@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'Harshit', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of', 'aren', 've', 'theirs', 'your', 'these', 'yourself', 'in', \"mustn't\", 'himself', 'other', \"you've\", 'his', 'had', \"wouldn't\", 'ain', 'at', 'such', 'off', 'hasn', 'ours', 'as', 'down', 'its', \"that'll\", 'during', 'once', \"mightn't\", \"you'll\", \"hadn't\", 'do', 'doesn', \"weren't\", 'yours', 'because', 'from', 'doing', 's', 're', 'shan', 'under', \"haven't\", 'you', \"you're\", 'm', 'are', 'the', 'too', 'we', 'only', 'won', \"won't\", 'has', 'yourselves', 'she', \"it's\", 'above', 'into', 'before', 'very', 'our', 'until', \"needn't\", 'not', 'or', 'her', 'just', 'nor', 'further', \"wasn't\", 'some', 'which', 'where', 'were', 'an', 'again', 'a', 'itself', 'out', 'haven', 'through', \"hasn't\", 'myself', \"you'd\", 't', 'how', 'and', 'they', 'here', 'up', 'most', 'am', 'with', 'him', \"couldn't\", 'no', \"shouldn't\", 'i', 'themselves', 'does', 'mightn', 'll', 'was', 'over', \"should've\", 'about', \"doesn't\", 'ourselves', 'what', 'then', 'mustn', 'on', \"don't\", 'wasn', 'own', 'don', 'if', 'by', 'couldn', 'needn', 'herself', 'there', 'both', 'this', 'it', 'have', 'hers', 'having', 'he', 'so', 'weren', 'between', 'than', 'why', 'will', 'now', 'did', 'them', 'after', 'all', 'to', 'can', 'more', 'that', 'is', \"aren't\", 'didn', \"isn't\", 'me', 'their', 'against', \"she's\", 'my', 'hadn', \"shan't\", 'd', 'shouldn', 'when', 'few', 'same', 'o', 'those', 'be', 'y', 'any', 'wouldn', 'but', 'each', 'ma', 'being', 'while', 'isn', 'been', 'should', 'whom', 'who', 'below', \"didn't\", 'for'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bothered', 'much']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text,stopwords):\n",
    "    useful_words = [w for w in text if w not in stopwords]\n",
    "    return useful_words\n",
    "text = \"i am not bothered about her very much\".split()\n",
    "useful_text = remove_stopwords(text,sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at Harshit@gmail.com\"\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]') #in bracker we can write all char or range of char\n",
    "useful_text = tokenizer.tokenize(sentence) #that we want to have in tokenised list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'e', 'n', 'd', 'a', 'l', 'l', 't', 'h', 'e', 'd', 'o', 'c', 'u', 'm', 'e', 'n', 't', 's', 'r', 'e', 'l', 'a', 't', 'e', 'd', 't', 'o', 'c', 'h', 'a', 'p', 't', 'e', 'r', 's', 'a', 't', 'H', 'a', 'r', 's', 'h', 'i', 't', '@', 'g', 'm', 'a', 'i', 'l', '.', 'c', 'o', 'm']\n"
     ]
    }
   ],
   "source": [
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'Harshit@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+') # + will now consider continous stream of char that\n",
    "useful_text = tokenizer.tokenize(sentence) # can be used as one\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming / Lemmatisation\n",
    "- Process that transforms particular words(verbs,plurals)into their radical form\n",
    "- Preserve the semantics of the sentence without increasing the number of unique tokens\n",
    "- Example - jumps, jumping, jumped, jump ==> jump\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "#Snowball Stemmer, Porter, Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('loving')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Snowball Stemmer\n",
    "ss = SnowballStemmer('english')\n",
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful\n",
      "beauti\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wn = WordNetLemmatizer()\n",
    "print(wn.lemmatize('beautiful'))\n",
    "print(ss.stem('beautiful')) #beauti is not a word!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Vocab & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Corpus - Contains 4 Documents, each document can have 1 or more sentences\n",
    "corpus = [\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people.',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n",
    "]\n",
    "#we have created a dummy corpus to work on!! It has 4 documents of different categories- sports\n",
    "#, politics, literature,movies . Now we have to build a vocab. vocab is the list of all unique\n",
    "# words in the entire corpus. To make features in form of numbers from text we have to convert \n",
    "# each document into a vector of numbers so for every document we initialise a vector of 0's \n",
    "# of size vocab and then for each document we look at all words of document. for example-India \n",
    "#.If in vocab india was present at index 7 then we increase count of index 7 by 1.Then we get\n",
    "# document in form of vector of numbers. in Vocab which word is stored at which index is stored\n",
    "# in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "vectorized_corpus = cv.fit_transform(corpus) #fit will train it on corpus and build a vocab\n",
    "#and transform will return vectorised corpus using vocab\n",
    "#returns a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'wins': 39, 'world': 41, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'win': 38, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 40, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_) #vocab in form of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorized_corpus)) \n",
    "# (0,6)  1 means in 0th document word at index 6 (wrt to vocab) has freq of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "[[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1\n",
      "  0 2 0 1 0 2]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      "  0 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
      "  0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()\n",
    "print(vectorized_corpus.size)\n",
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 32)\t3\n",
      "  (0, 20)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "[array(['the', 'nobel', 'laurate', 'won', 'hearts', 'of', 'people'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "# Reverse Mapping!\n",
    "numbers = vectorized_corpus[2]\n",
    "print(numbers)\n",
    "s = cv.inverse_transform(numbers)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization with  Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]') \n",
    "useful_text = tokenizer.tokenize(sentence)\n",
    "\n",
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    # Remove Stopwords\n",
    "    words = remove_stopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)\n",
    "vectorized_corpus = cv.fit_transform(corpus).toarray()\n",
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Test Data\n",
    "test_corpus = [\n",
    "        'Indian cricket rock !',        \n",
    "]\n",
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More ways to Create Features\n",
    "- Unigram - every word as a feature\n",
    "- Bigrams - every 2 unique consecutive word can be used as a single feature\n",
    "- Trigrams\n",
    "- n-grams \n",
    "- TF-IDF Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when it is useful to use bigram models over unigram?\n",
    "ex- \"this is not good movie\" - A unigram model may predict this as good review because words are jumbled and it is difficult to capture negation (not) but if we use bigram model and club not and good together \"not good\" we know it will be a bad review.\n",
    "or \"this is good movie but actor is not present\" -unigram model may classify it as a negative review because of not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1  = [\"this is good movie\"]\n",
    "sent_2 = [\"this is good movie but actor is not present\"]\n",
    "sent_3 = [\"this is not good movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 7, 'is good': 3, 'good movie': 2, 'movie but': 5, 'but actor': 1, 'actor is': 0, 'is not': 4, 'not present': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(ngram_range=(2,2)) #x,x mean x-gram\n",
    "vectorized_sent2=cv.fit_transform(sent_2).toarray()\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 20, 'is': 9, 'good': 6, 'movie': 14, 'but': 3, 'actor': 0, 'not': 17, 'present': 19, 'this is': 21, 'is good': 10, 'good movie': 7, 'movie but': 15, 'but actor': 4, 'actor is': 1, 'is not': 12, 'not present': 18, 'this is good': 22, 'is good movie': 11, 'good movie but': 8, 'movie but actor': 16, 'but actor is': 5, 'actor is not': 2, 'is not present': 13}\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer(ngram_range=(1,3)) #every word will be considered as a single feature and \n",
    "# every 2 adjacent and every 3 adjacent word will also be considered as a single feature. so \n",
    "# basically it will have features of unigram,bigram and trigram models\n",
    "vectorized_sent2=cv.fit_transform(sent_2).toarray()\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Normalisation\n",
    "\n",
    "- Avoid features that occur very often, becauase they contain less information\n",
    "- Information decreases as the number of occurences increases across different type of documents\n",
    "- So we define another term - term-document-frequency which associates a weight with every term\n",
    "- tf = term frequency , idf = inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1  = \"this is good movie\"\n",
    "sent_2 = \"this was good movie\"\n",
    "sent_3 = \"this is not good movie\"\n",
    "\n",
    "corpus = [sent_1,sent_2,sent_3]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]]\n",
      "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}\n"
     ]
    }
   ],
   "source": [
    "vc = tfidf.fit_transform(corpus).toarray()\n",
    "print(vc)\n",
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
