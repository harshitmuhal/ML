{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1TzuyeXQeOW"
   },
   "source": [
    "## Linear Math -Vector, Matrix, Tensor, Matrix Product, Inverse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNOFpLKQQy22"
   },
   "source": [
    "### Basic library, function import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bal2U_bhQ8PC",
    "outputId": "e987cc1f-8b37-4413-9ef4-7c92a3426231"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ow6t2_QmRA9O"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def vector_plot(vecs, xlim, ylim, cols=[\"#1190FF\", \"#FF9A13\"], alpha=1):\n",
    "    plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'red'})\n",
    "    plt.axvline(x=0, color='k', zorder=0)\n",
    "    plt.axhline(y=0, color='k', zorder=0)\n",
    "\n",
    "    for i in range(len(vecs)):\n",
    "        if (isinstance(alpha, list)):\n",
    "            alpha_i = alpha[i]\n",
    "        else:\n",
    "            alpha_i = alpha\n",
    "        x = np.concatenate([[0,0],vecs[i]])\n",
    "        plt.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                   alpha=alpha_i)\n",
    "        plt.ylim(-xlim, xlim)\n",
    "        plt.xlim(-ylim, ylim)\n",
    "        plt.grid()\n",
    "\n",
    "def plot_vector2d(vector2d, origin=[0, 0], **options):\n",
    "    return plt.arrow(origin[0], origin[1], vector2d[0], vector2d[1],\n",
    "                   head_width=0.2, head_length=0.3, length_includes_head=True,\n",
    "                   **options)\n",
    "\n",
    "def plot_transform(P_before, P_after, text_before, text_after, name, color=['#FF9A13', '#1190FF'], axis = [0, 5, 0, 4], arrows=False):\n",
    "    if arrows:\n",
    "        for vector_before, vector_after in zip(tf.transpose(P_before), tf.transpose(P_after)):\n",
    "            plot_vector2d(vector_before, color=\"#FF9A13\", linestyle=\"--\")\n",
    "            plot_vector2d(vector_after, color=\"#1190FF\", linestyle=\"-\")\n",
    "        plt.rc_context({'axes.edgecolor':'orange', 'xtick.color':'red', 'ytick.color':'red'})\n",
    "        plt.gca().add_artist(Polygon(tf.transpose(P_before), alpha=0.2))\n",
    "        plt.gca().add_artist(Polygon(tf.transpose(P_after), alpha=0.3, color=\"#FF9A13\"))\n",
    "        plt.text(-.25, 1, text_before, size=18, color=color[1])\n",
    "        plt.text(1.5, 0, text_after, size=18, color=color[0])\n",
    "        plt.title(name, color='w')\n",
    "        plt.axis(axis)\n",
    "        plt.grid()\n",
    "\n",
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "      `namedtuple` or combinations thereof.\n",
    "\n",
    "    Returns:\n",
    "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    return tf.nest.pack_sequence_as(tensors,[t.numpy() if tf.is_tensor(t) else t for t in tf.nest.flatten(tensors)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfApd5HCfl-A"
   },
   "source": [
    "Linear Algebra is a field of mathematics that deals with linear equations, linear functions, and expressions through matrices and vector spaces.\n",
    "\n",
    "Machine learning relies heavily on linear algebra, it is important to understand vectors and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epJ6SETkaofP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkfhrJTrRM6Y"
   },
   "source": [
    "### 1-1. Scalars, Vectors, Matrices and Tensor\n",
    "- **Saclars**: Single number\n",
    "- **Vectors**: Array of numbers\n",
    "  - Numbers are sorted in order, and each number can be identified in order by index\n",
    "  - Vector is an arrow that can express a quantity with both size and direction (the direction of the arrow is the size, the direction is the direction)\n",
    "- **Matrices**: Matrix is a two-dimensional array of numbers, each element identified by two indices instead of one\n",
    "  - If matrix A has height m and width n, then  $A\\ in\\ \\mathbb {R}^{m \\times n}$\n",
    "  - The elements of the matrix $A_{m,n}$ \n",
    "\n",
    "![Scalars, Vectors, Matrices and Tensors](https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/fig0201a.png)\n",
    "\n",
    "- **Tensor**: Array of numbers arranged in a regular grid with variable number of axes.A tensor is an array with more than 2 axis.\n",
    "  - $A_{i,j,k}$ :location $(i, j, k) $Tensor of $A$\n",
    "  - Vectors can be represented by three components: $(x, y, z)$\n",
    "\n",
    "![Tensors](https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/fig0201b.PNG)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAJzDOaBTgJp"
   },
   "source": [
    "$$\\color{Orange}{C=A+B\\ where\\ C_{i, j} = A_{i,j} + B_{i,j} \\tag{1}}$$\n",
    "- Tensor in Tensorflow\n",
    "  - Rank 0 Tensor: Scalar\n",
    "  - Rank 1 Tensor: Vector\n",
    "  - Rank 2 Tensor: Matrix\n",
    "  - Rank 3 Tensor: 3-Tensor\n",
    "  - Rank n Tensor: n-Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "d0RfWFwxe9rA",
    "outputId": "f2e1a4e9-f69e-4205-cb99-5f7d23fa34b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3x3 Rank 2 Tensor A: \n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]] \n",
      "\n",
      "3x3 Rank 2 Tensor B: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]] \n",
      "\n",
      "Rank 2 Tensor C with shape=(3, 3) and elements: \n",
      "[[ 2.  3.  4.]\n",
      " [ 5.  6.  7.]\n",
      " [ 8.  9. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# ones 3x3 rank 2 tensor\n",
    "rank_2_tensor_A = tf.ones([3,3], name='MatrixA')\n",
    "print(\"3x3 Rank 2 Tensor A: \\n{} \\n\".format(rank_2_tensor_A))\n",
    "\n",
    "# 3x3 rank 2 tensor\n",
    "rank_2_tensor_B = tf.constant([[1,2,3], [4,5,6], [7,8,9]], name='MatrixB', dtype=tf.float32)\n",
    "print(\"3x3 Rank 2 Tensor B: \\n{} \\n\".format(rank_2_tensor_B))\n",
    "\n",
    "# add two tensor\n",
    "rank_2_tensor_C = tf.add(rank_2_tensor_A, rank_2_tensor_B, name='MatrixC')\n",
    "print(\"Rank 2 Tensor C with shape={} and elements: \\n{}\".format(rank_2_tensor_C.shape, rank_2_tensor_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "4t5wjcF0gxO3",
    "outputId": "9ccc9bdc-7c49-413f-b323-3d95ac5c40fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x3 Rank 2 Tensor two_by_three: \n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]] \n",
      "\n",
      "Imcompatible shapes to add with two_by_three of shape (2, 3)  and 3x3 Rank 2 Tensor B of shape(3, 3)\n"
     ]
    }
   ],
   "source": [
    "two_by_three = tf.ones([2,3])\n",
    "print(\"2x3 Rank 2 Tensor two_by_three: \\n{} \\n\".format(two_by_three))\n",
    "try:\n",
    "    incompatible_tensor = tf.add(two_by_three, rank_2_tensor_B)\n",
    "except:\n",
    "    print(\"Imcompatible shapes to add with two_by_three of shape {}\\\n",
    "    and 3x3 Rank 2 Tensor B of shape{}\".format(two_by_three.shape, rank_2_tensor_B.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-23BIA9HiedS"
   },
   "source": [
    "You can add a scalar to the matrix or multiply the matrix by a scalar by performing the corresponding operation on each element of the matrix.$$\\color{orange}{D=a \\cdot B+c\\ where\\ D_{i,j}=a \\cdot B_{i,j}+c \\tag{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "sWL4m2gwCfU8",
    "outputId": "512d2fe5-0fc7-4af0-d338-257e5346820a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Rank 2 Tensor B: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]] \n",
      "\n",
      "Scalar a: 2.0 \n",
      "Rank 2 Tensor for aB: \n",
      "[[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]\n",
      " [14. 16. 18.]] \n",
      "\n",
      "Scalar c: 3.0 \n",
      "Rank 2 Tensor D = aB+c: \n",
      "[[ 5.  7.  9.]\n",
      " [11. 13. 15.]\n",
      " [17. 19. 21.]] )\n"
     ]
    }
   ],
   "source": [
    "# scalar a,c and Matrix B\n",
    "rank_0_tensor_a = tf.constant(2, name=\"scalar_a\", dtype=tf.float32)\n",
    "rank_2_tensor_B = tf.constant([[1,2,3], [4,5,6], [7,8,9]], name=\"MatrixB\", dtype=tf.float32)\n",
    "rank_0_tensor_c = tf.constant(3, name=\"scalar_c\", dtype=tf.float32)\n",
    "\n",
    "# aB\n",
    "multiply_scalar = tf.multiply(rank_0_tensor_a, rank_2_tensor_B)\n",
    "\n",
    "# aB+c\n",
    "rank_2_tensor_D = tf.add(multiply_scalar, rank_0_tensor_c, name=\"MatrixD\")\n",
    "\n",
    "print(\"\"\"Original Rank 2 Tensor B: \\n{0} \\n\\nScalar a: {1} \\n\\\n",
    "Rank 2 Tensor for aB: \\n{2} \\n\\nScalar c: {3} \\n\\\n",
    "Rank 2 Tensor D = aB+c: \\n{4} )\"\"\".format(rank_2_tensor_B, rank_0_tensor_a, \n",
    "                                          multiply_scalar, rank_0_tensor_c, rank_2_tensor_D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1PkE_1uC5W_"
   },
   "source": [
    "### Tanspose\n",
    "\n",
    " Transpose of a matrix is a matrix in which rows and columns are swapped along a diagonal line called the main diagonal.\n",
    "\n",
    "Change the $A$ matrix to $A\\ top$ and define as follows\n",
    "$$(A^\\top)_{i,j} = A_{j,i}$$\n",
    "\n",
    "In numpy there also exists Transpose of a tensor- It reverses the shape of the tensor.\n",
    "If shape was (x,y,z,d) then shape of transpose will be (d,z,y,x). However, you can also specify the axis as well in the transpose function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "yx-WgtDzQcHO",
    "outputId": "b3fa0282-1af6-4b06-e17e-ae57bfe6fc64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 2 Tensor E of shape: (2, 3) and elements: \n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Tanspose of Rank 2 Tensor E of shape: (3, 2) and elements: \n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "# rank 2 Matrix E\n",
    "rank_2_tensor_E = tf.constant([[1,2,3], [4,5,6]])\n",
    "\n",
    "# transpose Matrix E\n",
    "transpose_E = tf.transpose(rank_2_tensor_E, name=\"transposeE\")\n",
    "\n",
    "print(\"\"\"Rank 2 Tensor E of shape: {0} and elements: \\n{1}\\n\\\n",
    "Tanspose of Rank 2 Tensor E of shape: {2} and elements: \\n{3}\"\"\".format(rank_2_tensor_E.shape, rank_2_tensor_E,\n",
    "                                                                        transpose_E.shape, transpose_E))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNZhb-VfRh7J"
   },
   "source": [
    "### Broadcasting -\n",
    "broadcasting is not supported by python lists. It is supported by only numpy vectors/matrices/tensors.\n",
    "- Adding scalar to a vector adds that scalar to each element of vector\n",
    "- Adding vector to a matrix will add the vector to each row or to each column of matrix depending on the shape of vector\n",
    "\n",
    "In deep learning, another matrix with $C_{i,j} = A_{i,j}+b_j$ can be created by adding a matrix and a vector.\n",
    "That is, the vector b is added to each row of the matrix, and implicit copying of b to multiple locations is called **broadcasting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "xID7ARgnUGFt",
    "outputId": "2a29f28e-6587-4596-bcfc-0490c851cf1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 2 tensor A: \n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      " \n",
      "Rank 1 Tensor b: \n",
      "[[4.]\n",
      " [5.]\n",
      " [6.]]\n",
      "Rank 2 tensor F = A+b: \n",
      "[[5. 5. 5.]\n",
      " [6. 6. 6.]\n",
      " [7. 7. 7.]]\n"
     ]
    }
   ],
   "source": [
    "# rank 1 vector b\n",
    "rank_1_tensor_b = tf.constant([[4.], [5.], [6.]])\n",
    "# broadcast, F = A + b\n",
    "rank_2_tensor_F = tf.add(rank_2_tensor_A, rank_1_tensor_b, name=\"broadcastF\")\n",
    "\n",
    "print(\"\"\"Rank 2 tensor A: \\n{0}\\n \\nRank 1 Tensor b: \\n{1}\\\n",
    "\\nRank 2 tensor F = A+b: \\n{2}\"\"\".format(rank_2_tensor_A, rank_1_tensor_b, rank_2_tensor_F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnK4-q-RU3bN"
   },
   "source": [
    "### 1-2. Multiplying Matrices and Vectors\n",
    "To define matrix product of matrices $A$, $B$, and $A$, $A$ must have the same number of columns as $B$\n",
    "\n",
    "If the shape of $A$ is $m\\times n$ and the shape of $B$ is $n \\times p$, then the shape of $C$ is $m \\times p$\n",
    "$$\\color{orange}{C_{i,j}=\\sum_kA_{i,k}B_{k,j} \\tag{3}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "C0D_m8uBYEbS",
    "outputId": "d6078aff-5469-4a24-b1b7-30b3b3a2480f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: shape (2, 3) \n",
      "elements: \n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Matrix B: shape (3, 4) \n",
      "elements: \n",
      "[[1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]\n",
      " [1. 2. 3. 4.]]\n",
      "Matrix C: shape (2, 4) \n",
      "elements: \n",
      "[[ 3.  6.  9. 12.]\n",
      " [ 3.  6.  9. 12.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix A shape: (2,3) B shape: (3,4)\n",
    "mmv_matrix_A = tf.ones([2,3], name=\"matrix_A\")\n",
    "mmv_matrix_B = tf.constant([[1,2,3,4], [1,2,3,4], [1,2,3,4]], name=\"matrix_B\", dtype=tf.float32)\n",
    "\n",
    "# Matrix C: C=AB, C shape: (2,4)\n",
    "matrix_multiply_C = tf.matmul(mmv_matrix_A, mmv_matrix_B, name=\"matrix_multiply_C\")\n",
    "\n",
    "print(\"\"\"Matrix A: shape {0} \\nelements: \\n{1}\\\n",
    "\\n\\nMatrix B: shape {2} \\nelements: \\n{3}\\\n",
    "\\nMatrix C: shape {4} \\nelements: \\n{5}\"\"\".format(mmv_matrix_A.shape, mmv_matrix_A,\n",
    "                                                  mmv_matrix_B.shape, mmv_matrix_B,\n",
    "                                                  matrix_multiply_C.shape, matrix_multiply_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdX3EGpmZNqi"
   },
   "source": [
    "To get a matrix containing the product of individual elements, use **element wise production** or **Handmamard product** and write $A \\odot B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "aO5QPA0ta5ww",
    "outputId": "4b8778fd-216e-4eaa-be47-a441f17c1c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: shape (3, 3) \n",
      "elements: \n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Matrix A: shape (3, 3) \n",
      "elements: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "Matrix C: shape (3, 3) \n",
      "elements: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix A, B shape (3,3)\n",
    "element_matrix_A = tf.ones([3,3], name=\"element_matrix_A\")\n",
    "element_matrix_B = tf.constant([[1,2,3], [4,5,6], [7,8,9]], name=\"element_matrix_B\", dtype=tf.float32)\n",
    "\n",
    "# element wise product\n",
    "element_wise_C = tf.multiply(element_matrix_A, element_matrix_B, name=\"element_wise_C\")\n",
    "\n",
    "print(\"\"\"Matrix A: shape {0} \\nelements: \\n{1}\\\n",
    "\\n\\nMatrix A: shape {2} \\nelements: \\n{3}\\\n",
    "\\nMatrix C: shape {4} \\nelements: \\n{5}\"\"\".format(element_matrix_A.shape, element_matrix_A,\n",
    "                                                  element_matrix_B.shape, element_matrix_B,\n",
    "                                                  element_wise_C.shape, element_wise_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6EwvEXLUbbE_"
   },
   "source": [
    "![Dot Product](https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/fig0202b.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "xrvxI5Vgco6n",
    "outputId": "7a1e924b-3ba5-4b69-839e-8bb78fed2ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: shape (3, 3) \n",
      "elements: \n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "Matrix B: shape (3, 3) \n",
      "elements: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "Matrix C: shape (3, 3) \n",
      "elements: \n",
      "[[12. 15. 18.]\n",
      " [12. 15. 18.]\n",
      " [12. 15. 18.]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix A, B shape: (3,3)\n",
    "dot_matrix_A = tf.ones([3,3], name=\"dot_matrix_A\")\n",
    "dot_matrix_B = tf.constant([[1,2,3], [4,5,6], [7,8,9]], name=\"dot_matrix_B\", dtype=tf.float32)\n",
    "\n",
    "# Dot Product AB\n",
    "dot_product_C = tf.tensordot(dot_matrix_A, dot_matrix_B, axes=1, name=\"dot_product_C\")\n",
    "\n",
    "print(\"\"\"Matrix A: shape {0} \\nelements: \\n{1}\\\n",
    "\\n\\nMatrix B: shape {2} \\nelements: \\n{3}\\n\\\n",
    "Matrix C: shape {4} \\nelements: \\n{5}\"\"\".format(dot_matrix_A.shape, dot_matrix_A,\n",
    "                                                dot_matrix_B.shape, dot_matrix_B,\n",
    "                                                dot_product_C.shape, dot_product_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9_OGpi1c8W8"
   },
   "source": [
    "\n",
    "Some properties of matrix multiplication (variance properties): $$\\color{orange}{A(B+C)=AB+AC\\tag{4}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EaPDxI0LfB7g"
   },
   "outputs": [],
   "source": [
    "matrix_A = tf.constant([[1,2], [3,4]], name=\"matrix_a\")\n",
    "matrix_B = tf.constant([[5,6], [7,8]], name=\"matrix_b\")\n",
    "matrix_C = tf.constant([[9,1], [2,3]], name=\"matrix_c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "mUUaWCtVfcG2",
    "outputId": "f6aafc6c-49df-41c9-8927-e765deaa92c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "Matrix B: \n",
      "[[5 6]\n",
      " [7 8]] \n",
      "\n",
      "Matrix C: \n",
      "[[9 1]\n",
      " [2 3]]\n",
      "\n",
      "Distributive property is valid RHS: AB+AC: \n",
      "[[32 29]\n",
      " [78 65]] \n",
      "\n",
      "LHS: A(B+C): \n",
      "[[32 29]\n",
      " [78 65]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix A: \\n{} \\n\\nMatrix B: \\n{} \\n\\nMatrix C: \\n{}\\n\".format(matrix_A, matrix_B, matrix_C))\n",
    "\n",
    "# AB+AC\n",
    "distributive_RHS = tf.add(tf.matmul(matrix_A, matrix_B), tf.matmul(matrix_A, matrix_C), name=\"RHS\")\n",
    "\n",
    "# A(B+C)\n",
    "distributive_LHS = tf.matmul(matrix_A, (tf.add(matrix_B, matrix_C)), name=\"LHS\")\n",
    "\n",
    "predictor = tf.reduce_all(tf.equal(distributive_RHS, distributive_LHS))\n",
    "\n",
    "def true_print():\n",
    "    print(\"\"\"Distributive property is valid RHS: AB+AC: \\n{} \\n\\nLHS: A(B+C): \\n{}\"\"\".format(distributive_RHS, distributive_LHS))\n",
    "\n",
    "def false_print():\n",
    "    print(\"\"\"You Broke the Distributive Property of Matrix RHS: AB+AC: \\n{}\\\n",
    "  \\n\\nis NOT Equal to LHS: A(B+C): \\n{}\"\"\".format(distributive_RHS, distributive_LHS))\n",
    "\n",
    "tf.cond(predictor, true_print, false_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TctPeoMTh2J4"
   },
   "source": [
    "Some properties of matrix multiplication (association properties):\n",
    " $$\\color{orange}{A(BC)=(AB)C\\tag{5}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "17eV6P6si_Jv",
    "outputId": "7b76fc03-12db-48e8-c0ff-752c7c0c0eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "Matrix B: \n",
      "[[5 6]\n",
      " [7 8]] \n",
      "\n",
      "Matrix C: \n",
      "[[9 1]\n",
      " [2 3]]\n",
      "\n",
      "Assosiative property is valid RHS: (AB)C: \n",
      "[[215  85]\n",
      " [487 193]] \n",
      "\n",
      "LHS: A(BC): \n",
      "[[215  85]\n",
      " [487 193]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix A: \\n{} \\n\\nMatrix B: \\n{} \\n\\nMatrix C: \\n{}\\n\".format(matrix_A, matrix_B, matrix_C))\n",
    "# (AB)C\n",
    "assosiative_RHS = tf.matmul(tf.matmul(matrix_A, matrix_B), matrix_C)\n",
    "# A(BC)\n",
    "assosiative_LHS = tf.matmul(matrix_A, tf.matmul(matrix_B, matrix_C))\n",
    "predictor = tf.reduce_all(tf.equal(assosiative_RHS, assosiative_LHS))\n",
    "def true_print():\n",
    "    print(\"\"\"Assosiative property is valid RHS: (AB)C: \\n{} \\n\\nLHS: A(BC): \\n{}\"\"\".format(assosiative_RHS, assosiative_LHS))\n",
    "def false_print():\n",
    "    print(\"\"\"You Broke the Assosiative Property of Matrix RHS: (AB)C): \\n{}\\\n",
    "  \\n\\nis NOT Equal to LHS: A(BC): \\n{}\"\"\".format(assosiative_RHS, assosiative_LHS))\n",
    "tf.cond(predictor, true_print, false_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSXLrPc1kdJ4"
   },
   "source": [
    "Some properties of matrix multiplication $$\\color{orange}{AB \\neq BA\\tag{6}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ojpfGJyvli7c",
    "outputId": "c9d19d5a-4d5a-4897-dbf3-90b539c6dbc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "Matrix B: \n",
      "[[5 6]\n",
      " [7 8]]\n",
      "Matrix Multiplication is not commutative  (AB): \n",
      "[[19 22]\n",
      " [43 50]] \n",
      "\n",
      "LHS: (BA): \n",
      "[[23 34]\n",
      " [31 46]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix A: \\n{} \\n\\nMatrix B: \\n{}\".format(matrix_A, matrix_B))\n",
    "\n",
    "# Matrix A times B\n",
    "commutative_RHS = tf.matmul(matrix_A, matrix_B)\n",
    "\n",
    "# Matrix B times A\n",
    "commutative_LHS = tf.matmul(matrix_B, matrix_A)\n",
    "\n",
    "predictor = tf.logical_not(tf.reduce_all(tf.equal(commutative_RHS, commutative_LHS)))\n",
    "\n",
    "def true_print():\n",
    "    print(\"\"\"Matrix Multiplication is not commutative  (AB): \\n{} \\n\\nLHS: (BA): \\n{}\"\"\".format(commutative_RHS, commutative_LHS))\n",
    "\n",
    "def false_print():\n",
    "    print(\"\"\"You made Matrix Multipliccation commutative RHS: (AB): \\n{}\\\n",
    "  \\n\\n LHS: (BA): \\n{}\"\"\".format(commutative_RHS, commutative_LHS))\n",
    "\n",
    "tf.cond(predictor, true_print, false_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wqh738hLmVk0"
   },
   "source": [
    "(Transpose): $$\\color{orange}{(AB)^\\top = B^\\top A^\\top\\tag{7}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "o3Zi1B6KpD-I",
    "outputId": "2a4f2486-661e-4160-f7fc-1600d2b4f5a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "[[1 2]\n",
      " [3 4]] \n",
      "\n",
      "Matrix B: \n",
      "[[5 6]\n",
      " [7 8]]\n",
      "\n",
      "Transpose property is valid RHS: (AB)^T: \n",
      "[[19 43]\n",
      " [22 50]]  \n",
      "\n",
      "LHS: (B^T A^T): \n",
      "[[19 43]\n",
      " [22 50]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix A: \\n{} \\n\\nMatrix B: \\n{}\\n\".format(matrix_A, matrix_B))\n",
    "\n",
    "transpose_RHS = tf.transpose(tf.matmul(matrix_A, matrix_B))\n",
    "\n",
    "transpose_LHS = tf.matmul(matrix_B, matrix_A, transpose_a=True, transpose_b=True)\n",
    "\n",
    "predictor = tf.reduce_all(tf.equal(transpose_RHS, transpose_LHS))\n",
    "\n",
    "def true_print():\n",
    "    print(\"\"\"Transpose property is valid RHS: (AB)^T: \\n{}\\\n",
    "  \\n\\nLHS: (B^T A^T): \\n{}\"\"\".format(transpose_RHS, transpose_LHS))\n",
    "\n",
    "def false_print():\n",
    "    print(\"\"\"You Broken the Transpose property of Matrix RHS: (AB)^T: \\n{}\\\n",
    "  \\n\\nLHS: (B^TA^T): \\n{}\"\"\".format(transpose_RHS, transpose_LHS))\n",
    "\n",
    "tf.cond(predictor, true_print, false_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvU9udD1rg5Y"
   },
   "source": [
    "### 1-3. Identity and Inverse Matrices\n",
    "Linear algebra provides a powerful tool called **matrix inversion**.\n",
    "Analytical resolution of $Ax=b$ for many values ​​of $A$\n",
    "To explain matrix inversion, we need to first define the concept of **identity matrix**.\n",
    "The identity matrix is ​​a matrix that does not change the vector when multiplied by that matrix.\n",
    "$$\\color{orange}{I_n \\in \\mathbb{R}^{n \\times m} \\text{and}\\ \\forall x \\in \\mathbb{R}^n, I_nx = x \\tag{8} }$$\n",
    "\n",
    "The structure of the identity matrix is ​​that all items along the main diagonal are 1 and all items are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "Kiezd49DL4Ki",
    "outputId": "eb0f5ed6-6d7e-46c2-e1fe-76c4e8f1efe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity matrix I: \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Vector x: \n",
      "[[4.]\n",
      " [5.]\n",
      " [6.]]\n",
      "\n",
      "Matrix C from Ix: \n",
      "[[4.]\n",
      " [5.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": [
    "# matrix I\n",
    "identity_matrix_I = tf.eye(3,3, dtype=tf.float32, name='IdentityMatrixI')\n",
    "print(\"Identity matrix I: \\n{}\\n\".format(identity_matrix_I))\n",
    "\n",
    "iim_vector_x = tf.constant([[4], [5], [6]], name='Vector_x', dtype=tf.float32)\n",
    "print(\"Vector x: \\n{}\\n\".format(iim_vector_x))\n",
    "\n",
    "iim_matrix_C = tf.matmul(identity_matrix_I, iim_vector_x, name='MatrixC')\n",
    "print(\"Matrix C from Ix: \\n{}\".format(iim_matrix_C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE1QPOo9QEeB"
   },
   "source": [
    "**Matrix inverse** of $A$ is expressed as $A^{-1}$ and is defined as follows\n",
    "$$\\color{orange}{A^{-1}A = I_n\\tag{9}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tlo8leChTn9V"
   },
   "source": [
    "\n",
    "The equation $Ax=b$ can be solved as\n",
    "$$\\color{orange}{A^{-1}Ax=A^{-1}b \\\\\n",
    "I_nx=A^{-1}b \\\\\n",
    "x=A^{-1}b\\tag{10}}$$\n",
    "\n",
    "![Matrix Inverse](https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/fig0203a.PNG)\n",
    "\n",
    "Prove that a linear equation cannot have two solutions. (Eigher one or infinitely many solutions) ??\n",
    "\n",
    "Think of the columns of $\\textbf{A}$ as the different directions we can travel to from origin, then determine how many different ways we can follow to reach $\\textbf{b}$.\n",
    "\n",
    "$$ \\textbf{A}\\textbf{x} = \\sum_{i}x_{i}\\textbf{A}_{:,i}$$\n",
    "\n",
    "If $\\textbf{b}$ is in the span of the coumns of $\\textbf{A}$, then $\\textbf{A}\\textbf{x}=\\textbf{b}$ must have  a solution. In this particular case, the span is called the range or **column space** of $\\textbf{A}$.\n",
    "\n",
    "The requirement that column space of $\\textbf{A}$ be all of $\\mathbb{R}^{m}$ implies that $\\textbf{A}$ must have atleast m columns. Otherwise dimensionalty of column space would be less than m. A an example take $\\textbf{A}$ as a $3 \\times 2$ matrix, that means we only have two $x_{i}$'s which can vary to produce the output. The output will lie on a 2-D plane in the 3-D space. However, this is necessary and not sufficient condition for $\\textbf{A}\\textbf{x}=\\textbf{b}$ having a solution.\n",
    "\n",
    "Even if the matrix has 3 columns, if two of them are the same, output for $\\textbf{A}\\textbf{x}$ would lie on $\\mathbb{R}^{2}$. Thus the columns should also be linearly independent.\n",
    "\n",
    "For the matrix $\\textbf{A}$ to have inverse, it is required that the equation has exactly one solution for each value of $\\textbf{b}$. Thus the matrix should have **m linearly independent columns**.\n",
    "<br><br>\n",
    "**Using Gauss-Jordan Elimination to find Inverse**\n",
    "\n",
    "$$ \\textbf{E}\\begin{bmatrix} \\textbf{A} & \\textbf{I}\\end{bmatrix} = \\begin{bmatrix} \\textbf{I} & \\textbf{A} \\end{bmatrix}$$ \n",
    "\n",
    "where $\\textbf{E}$ is the tranformation matrix which converts the matrix $\\textbf{A}$ to $\\textbf{I}$ ie. it is $\\textbf{A}^{-1}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]]\n",
      "-2.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([[1,2],[4,5]])\n",
    "print(a)\n",
    "print(np.linalg.det(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "5FJ7V-eUb866",
    "outputId": "47d2706e-65d1-4303-9127-7f459d242ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]]\n",
      "[[-1.66666667  0.66666667]\n",
      " [ 1.33333333 -0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "#inverse\n",
    "a_inv=np.linalg.inv(a)\n",
    "print(a)\n",
    "print(a_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pseudo inverse - for non invertible matrices with |A|=0, pseudo inverse can be calculated instead of inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 9]\n",
      " [3 2]]\n",
      "[[-0.13333333  0.6       ]\n",
      " [ 0.2        -0.4       ]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[6,9],[3,2]])  # det(a) = 0 so inverse doesn't exist\n",
    "a_pseudo_inv=np.linalg.pinv(a)\n",
    "print(a)\n",
    "print(a_pseudo_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norm can be thought as a proxy for size of matrix.\n",
    "Norms are the set of functions mapping vectros to a set of non-negative values. The norm of a vector $\\textbf{x}$ measures the distance between origin to the point x in the space.\n",
    "\n",
    "$$ ||\\textbf{x}||_{p} = \\Big(\\sum_{i}|x_{i}|^{p}\\Big)^{\\frac{1}{p}}$$\n",
    "\n",
    "Formally, norm us any function that satisfies the following properties :-\n",
    "\n",
    "- f(**x**)=0 $\\implies$ x=0\n",
    "- f(**x**+**y**) $\\leq$ f(**x**)+f(**y**) (triangle inequality)\n",
    "- $\\forall \\alpha \\in \\mathbb{R}$, f($\\alpha$**x**)=$\\alpha$f(**x**) \n",
    "\n",
    "Most common norm is the $L^{2}(\\textbf{x})$ norm, also denoted as $||\\textbf{x}||$. $L^{2}(\\textbf{x})$ norm is the eucledian norm. $L^{2}(\\textbf{x})$ norm grows very slowly near the origin so it is not used generally in ML because near origin gradients are very small and we suffer from vanishing gradient problem. Hence when we need the distance value to increase quickly near the origin we use $L^{1}(\\textbf{x})$ norm. This is very important for some problems in machine learning where we need to differnciate the elements which are exactly 0 with other which are near to 0. $L^{1}(\\textbf{x})$ norm grows at the same rate in all the locations. \n",
    "\n",
    "There are other commonly used norms defined as following :-\n",
    "\n",
    "- $L^{0}(\\textbf{x})$ norm - Number of non-zero elements in a vector $\\textbf{x}$. Actually this violates the third requirement of being a norm. Still, it is mis-named as a norm.\n",
    "- $L^{\\infty}(\\textbf{x})$ norm - Max element in the vector $\\textbf{x}$\n",
    "$$||\\textbf{x}||_{\\infty} = max_{i} |x_{i}| $$\n",
    "- Frobenius Norm - $L^{2}$ norm with a martix $\\textbf{A}$ instead of a vector $\\textbf{x}$\n",
    "$$ ||\\textbf{A}||_{2} = \\sqrt{\\Big(\\sum_{i}\\sum_{j}|A_{i,j}|^{2}\\Big)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.57252408878007\n",
      "46.57252408878007\n",
      "46.0\n"
     ]
    }
   ],
   "source": [
    "vector=np.array([1,4,46,6])\n",
    "\n",
    "L2_norm=np.linalg.norm(vector,ord=2)\n",
    "print(L2_norm)\n",
    "\n",
    "L3_norm=np.linalg.norm(vector,ord=2)\n",
    "print(L3_norm)\n",
    "\n",
    "Linfinty_norm=np.linalg.norm(vector,ord=np.inf)\n",
    "print(Linfinty_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Kind of Matrices \n",
    "\n",
    "#### Upper Triangular Matrix\n",
    "\n",
    "#### Lower Triangular Matrix\n",
    "\n",
    "#### Diagonal Matrices\n",
    "\n",
    "A diagonal matrix, written as $\\textbf{D}$ is a matrix in which the individual entries are defined as follows :-\n",
    "\n",
    "$$\\textbf{D}_{i,j} \\neq 0 \\hspace{0.1in} \\forall i,j : i\\neq j$$\n",
    "$$\\textbf{D}_{i,j} = 0 \\hspace{0.1in} \\forall i,j : i=j$$\n",
    "\n",
    "This defination leads to many interesting properties as $\\textbf{D}\\textbf{x} = \\textbf{D}\\odot\\textbf{x}$. This means multiplying a diagonal matrix with another vector just requires the pointwise multiplication of the corresponding elements. Also, inverting a diagonal matrix requries less number of operations as \n",
    "\n",
    "$$\\textbf{D}^{-1} = \\begin{bmatrix} \\frac{1}{d_{1}} & \\dotsc & \\dotsc  & \\dotsc \\\\ \\dotsc & \\frac{1}{d_{2}} & \\dotsc  & \\dotsc \\\\ \\dotsc & \\dotsc & \\frac{1}{d_{i}} & \\dotsc \\\\ \\dotsc & \\dotsc & \\dotsc  & \\frac{1}{d_{n}} \\end{bmatrix}$$\n",
    "\n",
    "Diagnomal matrix need not be square. Although, non-square diagonal matrix does not have inverse, a multiplication by these matrices is still cheap. If number of rows is greater than number of columns :-\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0  \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\end{bmatrix} = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "concatenate zeros to the result. If the number of columns are greater than number of rows :-\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0  \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix} = \\begin{bmatrix} x_{1} \\\\ x_{2}  \\end{bmatrix}$$\n",
    "\n",
    "discard last elements of $\\textbf{x}$\n",
    "\n",
    "\n",
    "\n",
    "In Machine Learning, restricting some matrices to daigonal may result in a more efficient algorithm\n",
    "\n",
    "#### Symmetric Matrix\n",
    "\n",
    "A matrix which is equivalent to it's own tranpose is called symmetric matrix. \n",
    "\n",
    "$$ \\textbf{A} = \\textbf{A}^{T}$$\n",
    "\n",
    "In machine learning, symmetric matrix appear when it's entries are generated by the function having two arguments in which order of the arguments have no effect on the output of the function eg. distance metric\n",
    "\n",
    "#### Skew-Symmytric Matrix \n",
    "\n",
    "\n",
    "#### Orthogonal/Orthonormal Matrices \n",
    "\n",
    "A vector $\\textbf{x}$ and $\\textbf{y}$ are called orthogonal if $\\textbf{x}^{T}\\textbf{y}=0$. If both of these vectors have non-zero norm, it means that they are at 90 degrees angle to each other. If both of the vectors have a unit norm, they are called **orthonormal**.\n",
    "\n",
    "Orthonormal matrix is a square matrix in which both the rows and columns are mutually orthonormal to each other. For this reason :-\n",
    "\n",
    "$$ \\textbf{A}^{T}\\textbf{A} = \\textbf{A}\\textbf{A}^{T} = I $$\n",
    "\n",
    "This is a very important property as this implies that we can calculate the inverse of $\\textbf{A}$ very quickly ($\\textbf{A}^{-1} = \\textbf{A}^{T}$).\n",
    "\n",
    "#### Unitary Matrix\n",
    "\n",
    "#### Hermitian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a linear transformation ?\n",
    "\n",
    "To transform the vector space such that it preserves **certain properties** of the vector space structure.\n",
    "\n",
    "T : V $\\rightarrow$ W\n",
    "\n",
    "- $\\vec{0} \\rightarrow \\vec{0}$ ie. the null vector corresponding to vector space V should be mapped to the null vector in vector space W\n",
    "- The lines which are parallel in vector space V should remain parallel in the vector space W\n",
    "\n",
    "##### Geometric significance of linear transofmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra 2 - Solving Linear System of Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common approach in mathematics is to construct a set of objects and define rules to maniuplate these objects. This is known as algebra. \n",
    "\n",
    "One such approach is known as linear algebra. Basically, linear algebra is the study of vectors. During early introduction  to vectors in high school years, we were introduced to them through physics classes having fourmulas such as  $ \\hspace{0.1in} \\vec{f} = m.\\vec{a}$. Back then, we imagined vectors as a scalar which also has a direction. We understood them through a geometric interpretation with an arrow. The length of the arrow dentote the magnitude and the head of the arrow pointed towards the direction of the vector.\n",
    "\n",
    "Moving on to the fresher/sophomore year in university, we were introduced to vectors as an abstract concept. \n",
    "\n",
    "Finally, for people who have pursued degrees in engineering disciplines, vector appeared in another avatar $\\mathbb{R}^{n}$ or a list of n numbers. These list of number often represent some \"data\" which needs to be analyzed. \n",
    "\n",
    "In this course, we would refresh all of these interpretations and try to connect them in a coherent manner. We would use the \"school\" interpretation of vectors to help visualize and understand \"university-fresher\" and \"engineer\" representation. Another point to be noted is that this course is bit leaning towards the \"university-fresher\" representation of the matrix as it forms basis of linear algebra. Some resources pertaining other interpretations are listed below in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a vector?**\n",
    "\n",
    "Vector is a special entity which can be added together and multiplied by scalars to produce another object which is also a vector.\n",
    "\n",
    "- Geometric Vectors - Two geometric vectors $\\textbf{x},\\textbf{y}$ can be added leading to another vector $\\textbf{z}$. They can also be multiplied by a scalar $\\lambda \\in \\mathbb{R}$ and the result is again a vector. Therefor geometric vectors are instances of the vector concepts defined above.\n",
    "\n",
    "- Polynomials ??\n",
    "    - Addition of two polynomials result in another polynomial\n",
    "    - Multiplication of a polynomial with a scalar also results in a polynomial\n",
    "    - Most importantly, elements of $\\mathbb{R}^{n}$, the tuple of n real numbers are also vectors.\n",
    "    \n",
    "<img src=\"Vectors_Polynomials.png\" width=600 height=300 >    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Closure of space**\n",
    "\n",
    "One of the interesting ideas in algebra, or more generally in mathematics is \"closure\". It means what are the set of entities which can be obtained after performing the proposed operations. In the context of vectors, what are the set of vectors we obtain starting with a small set of vectors, adding them to each other and scaling them. This is called **vector space**.\n",
    "\n",
    "**The concept of vector space forms the basis of much of machine learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**System of linear equations and how to solve them ?**\n",
    "\n",
    "Why are we learning this ? \n",
    "\n",
    "Because it helps to :-\n",
    "\n",
    "- Determine if the set of vectors are linearly dependent\n",
    "- Determine the rank of a matrix\n",
    "- Determine the basis of vector space\n",
    "- Compute determinent of a matrix\n",
    "- Determine if a vector $\\textbf{b} \\in \\mathbb{R}^{m}$ is in the span of columns of $\\textbf{A} \\in \\mathbb{R}^{m\\times n}$ by solving $\\textbf{A}\\textbf{x}=\\textbf{b}$. If $m > n$, then then the column space has less number of dimensions than output space. COMPLETE THIS\n",
    "\n",
    "What would we learn in this section ?\n",
    "\n",
    "- Particular and general solutions to the equation\n",
    "- Importance of Row Echelon and Reduced Row Echelon Form of a matrix\n",
    "- Gauss Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of equations can have unique solution, no solution or infinitely many solutions.\n",
    "\n",
    "Set 1\n",
    "$$ x_{1} + x_{2} + x_{3} = 3  $$\n",
    "$$ x_{1} - x_{2} + 2x_{3} = 2 $$\n",
    "$$ 2x_{1} + 3x_{3} = 1 $$\n",
    "\n",
    "Set 2\n",
    "$$ x_{1} + x_{2} + x_{3} = 3  $$\n",
    "$$ x_{1} - x_{2} + 2x_{3} = 2 $$\n",
    "$$ x_{2} + x_{3} = 2 $$\n",
    "\n",
    "Set 3\n",
    "$$ x_{1} + x_{2} + x_{3} = 3  $$\n",
    "$$ x_{1} - 2x_{2} + x_{3} = 2 $$\n",
    "$$ 2x_{1} + 3x_{3} = 5 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{a}  = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n}\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Let us change the way we write the above equations\n",
    "\n",
    "\\begin{align}\n",
    "x_{1}\\begin{bmatrix}a_{11}\\\\\\vdots\\\\a_{m1}\\end{bmatrix}+x_{2}\\begin{bmatrix}a_{12}\\\\\\vdots\\\\a_{m2}\\end{bmatrix}+\\cdots+x_{n}\\begin{bmatrix}a_{1n}\\\\\\vdots\\\\a_{mn}\\end{bmatrix} = \\begin{bmatrix}b_{1}\\\\\\vdots\\\\b_{m}\\end{bmatrix} \\Leftrightarrow   \\begin{bmatrix} a_{11} & \\cdots & a_{1n}\\\\ \\vdots & \\vdots & \\vdots \\\\ a_{m1} & \\cdots & a_{mn}\\end{bmatrix} \\begin{bmatrix} x_{1}\\\\ \\vdots \\\\ x_{n}\\end{bmatrix}=\\begin{bmatrix} b_{1}\\\\ \\vdots \\\\ b_{n}\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "On the right side is something we call the compact representation of linear system of equations. We can write it as :-\n",
    "\n",
    "$$ \\textbf{A}\\textbf{x} = \\textbf{b}$$\n",
    "\n",
    "Note that this representation multiplies columns of the matrix with the individual entries in vector $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no rigid format to write the variable vector to the right of coefficient matrix. We can also write the variable vector to the left of the coefficient vector as below :-\n",
    "\n",
    "$$ x_{1}\\begin{bmatrix} a_{11} & \\cdots & a_{1m} \\end{bmatrix} + x_{2}\\begin{bmatrix} a_{21} & \\cdots & a_{2m} \\end{bmatrix} + \\cdots + x_{n} \\begin{bmatrix} a_{11} & \\cdots & a_{1m} \\end{bmatrix} \\Leftrightarrow   \\begin{bmatrix} x_{1} & \\cdots & x_{n} \\end{bmatrix} \\begin{bmatrix} a_{11} & \\cdots & a_{1n} \\\\ \\vdots & \\vdots & \\vdots \\\\ a_{m1} & \\cdots & a_{mn}\\end{bmatrix} = \\begin{bmatrix} b_{1} & \\cdots & b_{n}\\end{bmatrix}$$\n",
    "\n",
    "Note that this representation involves multiplying rows of the matrix with individual entries of the transpose of the vector $\\textbf{x}$. In a compact representation, we can write it as :-\n",
    "\n",
    "$$ \\textbf{x}^{T}\\textbf{A} = \\textbf{b}^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's digress a bit towards something called a **matrix**. A matrix is a set of vector which can be used to compactly represent linear equations and linear transformations of a vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solving the linear system of equations**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given a system of linear equations :-\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} 1 & 0 & 8 & -4 \\\\ 0 & 1 & 2 & 12 \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\\\x_{4}\\end{bmatrix} = \\begin{bmatrix}42\\\\8\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Notice the first two columns of this linear equation, do you see something there ?? We need to find the values $x_{i}$'s such that the following is satisfied :-\n",
    "\n",
    "$$ \\sum_{i=1}^{4}x_{i}\\textbf{c}_{i} = \\textbf{b}$$ where $\\textbf{c}_{i}$ are the columns of $\\textbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Particular Solution**\n",
    "\n",
    "A solution to the equation can be found by taking 42 times first column and 8 times the second column.\n",
    "\n",
    "\\begin{align}\n",
    "b=\\begin{bmatrix} 42 \\\\ 8\\end{bmatrix} = 42\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}+8\\begin{bmatrix} 0 \\\\ 1\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "So one **particular solution** to the equation is $[42,8,0,0]^{T}$. However, this is not the only solution to the above equations. So what do we need to do to find all the other solutions to the above equation ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Solution**\n",
    "\n",
    "To find the other solutions, we need to generate 0 using the existing columns in a non-trivial way. Adding 0 to the **particular solution** does not change the solution. \n",
    "\n",
    "To do so, we can express any other column as the sum of first two columns. For the third column, we can write\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} 8 \\\\ 2\\end{bmatrix} = 8\\begin{bmatrix} 1 \\\\ 0\\end{bmatrix}+2\\begin{bmatrix} 0 \\\\ 1\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "This means that $8\\textbf{c}_{1}+2\\textbf{c}_{2}-1\\textbf{c}_{3}+0\\textbf{c}_{4}= \\textbf{0}$ and $[8,2,-1,0]^{T}$ is a solution of $\\textbf{A}\\textbf{x}=\\textbf{0}$. In fact, any scaling of $[8,2,-1,0]^{T}$ by $\\lambda_{1} \\in \\mathbb{R}$ produces the $\\textbf{0}$ vector.\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} 1 & 0 & 8 & -4 \\\\ 0 & 1 & 2 & 12 \\end{bmatrix} \\begin{pmatrix} \\lambda_{1}\\begin{bmatrix}8\\\\2\\\\-1\\\\0\\end{bmatrix} \\end{pmatrix}= \\begin{bmatrix}0\\\\0\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we can denote the fourth column as the sum of first two columns.\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} 1 & 0 & 8 & -4 \\\\ 0 & 1 & 2 & 12 \\end{bmatrix} \\begin{pmatrix} \\lambda_{1}\\begin{bmatrix}-4\\\\12\\\\0\\\\-1\\end{bmatrix} \\end{pmatrix}= \\begin{bmatrix}0\\\\0\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Finally, putting eveything together, we obtain a general solution to the linear system of equations which we call **general solution**.\n",
    "\n",
    "\\begin{align}\n",
    "x=\\begin{bmatrix}42\\\\8\\\\0\\\\0\\end{bmatrix} + \\begin{pmatrix} \\lambda_{1}\\begin{bmatrix}8\\\\2\\\\-1\\\\0\\end{bmatrix} \\end{pmatrix} + \\begin{pmatrix} \\lambda_{1}\\begin{bmatrix}-4\\\\12\\\\0\\\\-1\\end{bmatrix} \\end{pmatrix}, \\lambda_{1},\\lambda_{2} \\in \\mathbb{R}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps for finding the general solution of a equation**\n",
    "\n",
    "- Find a particular solution to $\\textbf{A}\\textbf{x}=\\textbf{b}$\n",
    "- Find all solutions to $\\textbf{A}\\textbf{x}=\\textbf{0}$\n",
    "- Combine step 1 and 2 to get a general solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given any other system of equation, the above method may not directly apply on it because it may not have a convinent form that allowed us to find a particular and general solution by inspection. So, we need a definate way of transforming linear equation system into simple form. This is called **Gaussian Elimination**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go onto learning about Gaussian elimination, we need to know a key idea - **Elementary Transformations**. These transformations keep the solution to the set equations the same.\n",
    "\n",
    "- Exchange of two equations (or row in the matrix representing the equation system)\n",
    "- Multiplication of an equation with a constant $\\lambda \\in \\mathbb{R} \\setminus \\{0\\}$\n",
    "- Addition of an equation(row) to another equation(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find the solution for the given linear system of equations :-\n",
    "\n",
    "$$ -2x_{1} + 4x_{2} - 2x_{3} - x_{4} + 4x_{5} = -3 $$\n",
    "$$ 4x_{1} - 8x_{2} + 3x_{3} - 3x_{4} + x_{5} = 2 $$\n",
    "$$ x_{1} - 2x_{2} + x_{3} - x_{4} + x_{5} = 0 $$\n",
    "$$ x_{1} - 2x_{2} - 3x_{4} + 4x_{5} = a$$\n",
    "\n",
    "where $a \\in \\mathbb{R}$\n",
    "\n",
    "Solve using augmented matrix. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}-2 & 4 & -2 & -1 & 4 &  -3\\\\  4 & -8 & 3 & -3 & 1 &  2\\\\  1 & -2 & 1 & -1 & 1 &  0\\\\ 1 & -2 & 0 & -3 & 4 &  a\\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We should try to apply the above three operations in such a way that we get a structure \"simiar\" to that of the previous equation.\n",
    "\n",
    "Step 1 Take the row with least first element to the top row. Here we exchange the first row with the third row. $R_{1} \\leftrightarrow R_{3}$ \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}  1 & -2 & 1 & -1 & 1 &  0 \\\\  4 & -8 & 3 & -3 & 1 &  2\\\\ -2 & 4 & -2 & -1 & 4 &  -3\\\\ 1 & -2 & 0 & -3 & 4 &  a\\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Step 2. Now we can subtract rest of the three rows by first row such that their first element becomes 0. $R_{2} \\rightarrow R_{2}-4R_{1}$, $R_{3} \\rightarrow R_{3}+2R_{1}$, $R_{4} \\rightarrow R_{4}-R_{1}$ \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}  1 & -2 & 1 & -1 & 1 &  0 \\\\  0 & 0 & -1 & 1 & -3 &  2\\\\ 0 & 0 & 0 & -3 & 6 &  -3\\\\ 0 & 0 & -1 & -2 & 3 &  a\\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Step 3. Again we see that both row 2 and row 4 has same first non-zero element ie -1. Hence, $R_{4} \\rightarrow R_{4} - R_{2}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}  1 & -2 & 1 & -1 & 1 &  0 \\\\  0 & 0 & -1 & 1 & -3 &  2\\\\ 0 & 0 & 0 & -3 & 6 &  -3\\\\ 0 & 0 & 0 & -3 & 6 &  a-2\\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Step 4. Complete rest of the reduction procedure on your own. Finally you would have the following matrix :-\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}  1 & -2 & 1 & -1 & 1 &  0 \\\\  0 & 0 & -1 & 1 & 3 &  -2\\\\ 0 & 0 & 0 & 1 & -2 &  1\\\\ 0 & 0 & 0 & 0 & 0 &  a+1\\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "But still, we havn't done anything to solve the equation. We have just reduced it to the form which can be solved conveniently. This is called row echelon form (REF). Reverting back to the equation format, we get :-\n",
    "\n",
    "$$ x_{1} - 2x_{2} +x_{3}-x_{4}+x{5} = 0$$\n",
    "$$ x_{3}-x_{4}+3x_{5} = -2$$\n",
    "$$ x_{4}-2x_{5}=1$$\n",
    "$$ 0 = a+1$$\n",
    "\n",
    "**Can you find the particular and general solution to the equation now ?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pivot and the Staircase Structure**\n",
    "\n",
    "The leading coefficient of the row is called pivot and is always strictly to the right of the leading coefficient of the row above it. This makes sure that equation system in row echelon form has \"staircase structure\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Echelon Form**\n",
    "\n",
    "- All the nonzero rows are above any rows of all zeros . If there are rows with all zero elements, they must belong to the bottom of the matrix.\n",
    "- The leading coefficient of nonzero row is always strictly to the right of the leading coefficient of the row above it.\n",
    "\n",
    "The variables correspopnding to the pivots in row echelon form are called **basic variables**. Other variables are called **free variables**. In the example above, $x_{1},x_{3},x_{4}$ are basic variables and $x_{2},x_{5}$ are free variables.\n",
    "\n",
    "**What are the benefits of reduced row echelon form ?**\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}  1 & -2 & 1 & -1 & 1 &  0 \\\\  0 & 0 & -1 & 1 & -3 &  -2\\\\ 0 & 0 & 0 & 1 & -2 &  1\\\\ 0 & 0 & 0 & 0 & 0 &  a+1\\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Let's look at the elements corresponding to the columns which have pivot variables. This means that the second and fourth column can be removed.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}  1  & 1 & -1 &  0 \\\\  0 & 1  & -1 &  -2\\\\ 0 & 0  & 1 &  1\\\\ 0 & 0  & 0 &  a+1\\\\ \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We can express the right hand side of the equation system using the pivot columns, such that $b=\\sum_{i=1}^{P}\\lambda_{i}p_{i}$ where $p_{i}$ are the pivot columns. we can determine $\\lambda_{i}$ in a convenient manner if we start from the right most pivot column and work our way towards the left. For the example above, we try to find $\\lambda_{1},\\lambda_{2},\\lambda_{3}$ such that\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_{1}\\begin{bmatrix}1\\\\0\\\\0\\\\0\\end{bmatrix}+\\lambda_{2}\\begin{bmatrix}1\\\\1\\\\0\\\\0\\end{bmatrix}+\\lambda_{3}\\begin{bmatrix}-1\\\\-1\\\\1\\\\0\\end{bmatrix} = \\begin{bmatrix}0\\\\-2\\\\1\\\\0\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Now, working from the right-most pivot column, we can work our way towards the left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduced Row Echelon Form**\n",
    "\n",
    "An equation is set to be in reduced row echelon form when :-\n",
    "\n",
    "- It is in row echelon form\n",
    "- Every pivot must be 1\n",
    "- The pivot is only non-zero entry in its column\n",
    "\n",
    "Reduced Row Echelon Form helps us in finding general solution to the set of equations. The key for finding the seolutions of $\\textbf{A}\\textbf{x}=\\textbf{0}$ is to look at the *non-pivot columns* which we need to express as a linear combination of the pivot columns. The RREF makes this easier through as we can express the non-pivot columns in terms of the sums and multiples of pivot columns on their left. As an example take the following matrix :-\n",
    "\n",
    "\\begin{align}\n",
    "A=\\begin{bmatrix}1 & 3 & 0 & 0 & 3 \\\\ 0 & 0 & 1 & 0 & 9 \\\\ 0 & 0 & 0 & 1 & -4\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The above matrix is in RREF. We can express the second column as 3 times the first column; in the same way, fifth column can be expressed as 3 times the first column, 9 times the third column and -4 times the fourth column. Hence the solution to $\\textbf{A}\\textbf{x}=\\textbf{0}$ are :-\n",
    "\n",
    "\\begin{align}\n",
    "x = \\lambda_{1}\\begin{bmatrix}3\\\\-1\\\\0\\\\0\\\\0\\end{bmatrix}+\\lambda_{2}\\begin{bmatrix}3\\\\0\\\\9\\\\-4\\\\1\\end{bmatrix} \\lambda_{1},\\lambda_{2} \\in \\mathbb{R}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the equation  $\\textbf{A}\\textbf{x}=\\textbf{0}$ is called the null space of matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Solve the system of equations 3 * x0 + x1 = 9 and x0 + 2 * x1 = 8:\n",
    "\n",
    "a = np.array([[3,1], [1,2]])\n",
    "b = np.array([9,8])\n",
    "x = np.linalg.solve(a, b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOoZq3rZypIpcxzZaFQt5T7",
   "collapsed_sections": [],
   "name": "LinearAlgebra_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
